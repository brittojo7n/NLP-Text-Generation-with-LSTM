{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install wikipedia-api"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acxsW15GE3MH",
        "outputId": "7773ef55-8e00-48d8-99c7-70f7d2212288"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from wikipedia-api) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->wikipedia-api) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipediaapi\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "qn-NQypSJMqf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_formatted_lines(page_title):\n",
        "    user_agent = \"Mozilla/5.0 (https://github.com/brittojo7n/ProcessExplorer)\"\n",
        "    headers = {'User-Agent': user_agent}\n",
        "\n",
        "    wiki_wiki = wikipediaapi.Wikipedia('en', headers=headers)\n",
        "    page = wiki_wiki.page(page_title)\n",
        "\n",
        "    if not page.exists():\n",
        "        print(\"Page not found.\")\n",
        "        return []\n",
        "\n",
        "    formatted_lines = []\n",
        "\n",
        "    for section in page.sections:\n",
        "        for line in section.text.split('\\n'):\n",
        "            if line.strip():\n",
        "                formatted_lines.append(line.strip())\n",
        "\n",
        "    return formatted_lines"
      ],
      "metadata": {
        "id": "wsTQP5eMJOQx"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "    input_sequences = []\n",
        "    for line in lines:\n",
        "        tokenized_line = tokenizer.texts_to_sequences([line])[0]\n",
        "        for i in range(1, len(tokenized_line)):\n",
        "            n_gram_sequence = tokenized_line[:i+1]\n",
        "            input_sequences.append(n_gram_sequence)\n",
        "\n",
        "    max_sequence_length = max([len(seq) for seq in input_sequences])\n",
        "    input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='pre')\n",
        "\n",
        "    X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
        "\n",
        "    return X, y, total_words, max_sequence_length, tokenizer"
      ],
      "metadata": {
        "id": "Je8UN8tvHmsr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(total_words, max_sequence_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(total_words, 50, input_length=max_sequence_length-1))\n",
        "    model.add(LSTM(100, return_sequences=True))\n",
        "    model.add(LSTM(100))\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "hNvqUYHyJS4_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, X, y, epochs=10, batch_size=32):\n",
        "    model.fit(X, y, epochs=epochs, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "8hKmB1daJUwG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_content(model, tokenizer, seed_text, max_sequence_length, num_words=50):\n",
        "    for _ in range(num_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
        "        predicted_index = np.argmax(predicted_probs)\n",
        "\n",
        "        output_word = \"\"\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_index:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \" + output_word\n",
        "\n",
        "    return seed_text"
      ],
      "metadata": {
        "id": "dE2-8X-dI0Qr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    page_title = \"Python (programming language)\"\n",
        "    lines = get_formatted_lines(page_title)\n",
        "\n",
        "    X, y, total_words, max_sequence_length, tokenizer = preprocess_data(lines)\n",
        "\n",
        "    model = build_model(total_words, max_sequence_length)\n",
        "    train_model(model, X, y)\n",
        "\n",
        "    seed_text = \"Python is\"\n",
        "    generated_content = generate_content(model, tokenizer, seed_text, max_sequence_length)\n",
        "    print(generated_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IURs5fg8Ix0h",
        "outputId": "b6af9c68-c291-46ab-ac80-cce4b1f936d8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "98/98 [==============================] - 16s 127ms/step - loss: 6.5993 - accuracy: 0.0411\n",
            "Epoch 2/10\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 6.1699 - accuracy: 0.0302\n",
            "Epoch 3/10\n",
            "98/98 [==============================] - 5s 53ms/step - loss: 6.1239 - accuracy: 0.0360\n",
            "Epoch 4/10\n",
            "98/98 [==============================] - 4s 41ms/step - loss: 6.0730 - accuracy: 0.0356\n",
            "Epoch 5/10\n",
            "98/98 [==============================] - 5s 54ms/step - loss: 5.9494 - accuracy: 0.0443\n",
            "Epoch 6/10\n",
            "98/98 [==============================] - 4s 36ms/step - loss: 5.8445 - accuracy: 0.0504\n",
            "Epoch 7/10\n",
            "98/98 [==============================] - 3s 28ms/step - loss: 5.7740 - accuracy: 0.0520\n",
            "Epoch 8/10\n",
            "98/98 [==============================] - 3s 34ms/step - loss: 5.7122 - accuracy: 0.0555\n",
            "Epoch 9/10\n",
            "98/98 [==============================] - 4s 39ms/step - loss: 5.6414 - accuracy: 0.0565\n",
            "Epoch 10/10\n",
            "98/98 [==============================] - 3s 33ms/step - loss: 5.5681 - accuracy: 0.0587\n",
            "Python is python and python and python and python and python and python and python and python and python and python and python and python and python and python and python and python and python and python and python and python and python and python and python and python and python and\n"
          ]
        }
      ]
    }
  ]
}